{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NEREL_to_mrc_style_preprocess.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkIJJ-JEMIH0",
        "outputId": "14253cc1-1477-4999-a2f0-a03d23bea6f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2kzXb0kb3FG",
        "outputId": "6d2a2e6b-2bdd-4e8b-d7cf-6ce93db6150a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convert 1213 samples to 35177 samples and save to /content/drive/MyDrive/Diploma_paper/NEREL_MRC/mrc-ner.dev\n",
            "Convert 9431 samples to 273499 samples and save to /content/drive/MyDrive/Diploma_paper/NEREL_MRC/mrc-ner.train\n",
            "Convert 1288 samples to 37352 samples and save to /content/drive/MyDrive/Diploma_paper/NEREL_MRC/mrc-ner.test\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "def convert_file(input_file, output_file, tag2query_file):\n",
        "    all_data = json.load(open(input_file))\n",
        "    tag2query = json.load(open(tag2query_file))\n",
        "\n",
        "    output = []\n",
        "    origin_count = 0\n",
        "    new_count = 0\n",
        "\n",
        "    for data in all_data:\n",
        "      for elem in data:\n",
        "          origin_count += 1\n",
        "          context = elem[\"context\"]\n",
        "          label2positions = elem[\"label\"]\n",
        "          for tag_idx, (tag, query) in enumerate(tag2query.items()):\n",
        "              positions = label2positions.get(tag, [])\n",
        "              mrc_sample = {\n",
        "                  \"context\": context,\n",
        "                  \"query\": query,\n",
        "                  \"start_position\": [int(x.split(\",\")[0]) for x in positions],\n",
        "                  \"end_position\": [int(x.split(\",\")[1]) for x in positions],\n",
        "                  \"qas_id\": f\"{origin_count}.{tag_idx}\"\n",
        "              }\n",
        "              output.append(mrc_sample)\n",
        "              new_count += 1\n",
        "\n",
        "    json.dump(output, open(output_file, \"w\"), ensure_ascii=False, indent=2)\n",
        "    print(f\"Convert {origin_count} samples to {new_count} samples and save to {output_file}\")\n",
        "\n",
        "for file in ['dev', 'train', 'test']:\n",
        "  input_file = f'/content/drive/MyDrive/Diploma_paper/NEREL_01/{file}.json'\n",
        "  output_file = f'/content/drive/MyDrive/Diploma_paper/NEREL_MRC/mrc-ner.{file}'\n",
        "  tag2query_file = f'/content/drive/MyDrive/Diploma_paper/mrc_preprocess_wanglaiki/mrc-wanglaiki/data_preprocess/queries/ent_nerel.json'\n",
        "  convert_file(input_file, output_file, tag2query_file)\n",
        "\n",
        "\n",
        "# def main():\n",
        "#     genia_raw_dir = \"/mnt/mrc/genia/genia_raw\"\n",
        "#     genia_mrc_dir = \"/mnt/mrc/genia/genia_raw/mrc_format\"\n",
        "#     tag2query_file = \"queries/genia.json\"\n",
        "#     os.makedirs(genia_mrc_dir, exist_ok=True)\n",
        "#     for phase in [\"train\", \"dev\", \"test\"]:\n",
        "#         old_file = os.path.join(genia_raw_dir, f\"{phase}.genia.json\")\n",
        "#         new_file = os.path.join(genia_mrc_dir, f\"mrc-ner.{phase}\")\n",
        "#         convert_file(old_file, new_file, tag2query_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DjLlBeBWG9NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tp9u13i9m1NL",
        "outputId": "499c495a-d0db-475f-f7bc-6dee7647779f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "all_data = json.load(open('/content/drive/MyDrive/Diploma_paper/NEREL_MRC/shan_train.json', encoding=\"utf-8\"))\n",
        "\n",
        "item = 0 \n",
        "def preprocess(item):\n",
        "  data = all_data[item]\n",
        "\n",
        "  #print(f'data {data}', flush=True)\n",
        "  #print('Passed 0', flush=True)\n",
        "  #print(f'tokenizer {tokenizer}', flush=True)\n",
        "  qas_id = data.get(\"qas_id\", \"0.0\")\n",
        "  sample_idx, label_idx = qas_id.split(\".\")\n",
        "  sample_idx = torch.LongTensor([int(sample_idx)])\n",
        "  label_idx = torch.LongTensor([int(label_idx)])\n",
        "\n",
        "  query = data[\"query\"]\n",
        "  context = data[\"context\"]\n",
        "  start_positions = data[\"start_position\"]\n",
        "  end_positions = data[\"end_position\"]\n",
        "\n",
        "  # if self.is_chinese:\n",
        "  #     context = \"\".join(context.split())\n",
        "  #     end_positions = [x+1 for x in end_positions]\n",
        "  # else:\n",
        "  # add space offsets\n",
        "  words = context.split(' ')\n",
        "  start_positions = [x + sum([len(w) for w in words[:x]]) for x in start_positions]\n",
        "  end_positions = [x + sum([len(w) for w in words[:x + 1]]) for x in end_positions]\n",
        "\n",
        "  query_context_tokens = tokenizer.encode(query, context, add_special_tokens=True)\n",
        "  tokens = query_context_tokens.ids\n",
        "  type_ids = query_context_tokens.type_ids\n",
        "  offsets = query_context_tokens.offsets\n",
        "\n",
        "  # find new start_positions/end_positions, considering\n",
        "  # 1. we add query tokens at the beginning\n",
        "  # 2. word-piece tokenize\n",
        "  origin_offset2token_idx_start = {}\n",
        "  origin_offset2token_idx_end = {}\n",
        "  for token_idx in range(len(tokens)):\n",
        "      # skip query tokens\n",
        "      if type_ids[token_idx] == 0:\n",
        "          continue\n",
        "      token_start, token_end = offsets[token_idx]\n",
        "      # skip [CLS] or [SEP]\n",
        "      if token_start == token_end == 0:\n",
        "          continue\n",
        "      origin_offset2token_idx_start[token_start] = token_idx\n",
        "      origin_offset2token_idx_end[token_end] = token_idx\n",
        "  #print(f'origin_offset2token_idx_start {origin_offset2token_idx_start}', flush=True)\n",
        "  #print(f'origin_offset2token_idx_end  {origin_offset2token_idx_end}', flush=True)\n",
        "  #print(f'start_positions {start_positions} end_positions {end_positions}', flush=True)\n",
        "  new_start_positions = [origin_offset2token_idx_start[start] for start in start_positions]\n",
        "  #print(f'Passed 1', flush=True)\n",
        "  new_end_positions = [origin_offset2token_idx_end[end] for end in end_positions]\n",
        "  #print(f'Passed 2', flush=True)"
      ],
      "metadata": {
        "id": "XPFnfOmkm1U8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}